{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4aaa3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\"./data\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(\"./data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_subset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def to_numpy_loader(loader):\n",
    "  for images, labels in loader:\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    yield images.numpy(), labels.numpy()\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "  out = np.zeros((y.shape[0], num_classes), dtype=np.float32)\n",
    "  out[np.arange(y.shape[0]), y] = 1.0\n",
    "  return out\n",
    "\n",
    "def normalize(x):\n",
    "  return x.astype(np.float32)\n",
    "\n",
    "class NeuralNetwork:\n",
    "  def __init__(self, input_size, hidden_sizes, output_size, activation=\"relu\", lr=0.01):\n",
    "    self.activation_name = activation\n",
    "    self.lr = lr\n",
    "    self.layers = [input_size] + hidden_sizes + [output_size]\n",
    "    rng = np.random.default_rng()\n",
    "    self.weights = []\n",
    "    self.biases = []\n",
    "    for i in range(len(self.layers) - 1):\n",
    "      in_dim = self.layers[i]\n",
    "      out_dim = self.layers[i + 1]\n",
    "      w = rng.standard_normal((in_dim, out_dim)) * np.sqrt(2 / in_dim)\n",
    "      b = np.zeros((1, out_dim), dtype=np.float32)\n",
    "      self.weights.append(w.astype(np.float32))\n",
    "      self.biases.append(b)\n",
    "\n",
    "  def activation(self, x):\n",
    "    if self.activation_name == \"relu\":\n",
    "      return np.maximum(0, x)\n",
    "    if self.activation_name == \"sigmoid\":\n",
    "      return 1 / (1 + np.exp(-x))\n",
    "    return np.tanh(x)\n",
    "\n",
    "  def activation_grad(self, x):\n",
    "    if self.activation_name == \"relu\":\n",
    "      return (x > 0).astype(np.float32)\n",
    "    if self.activation_name == \"sigmoid\":\n",
    "      s = 1 / (1 + np.exp(-x))\n",
    "      return s * (1 - s)\n",
    "    t = np.tanh(x)\n",
    "    return 1 - t * t\n",
    "\n",
    "  def softmax(self, x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    a = x\n",
    "    self.zs = []\n",
    "    self.as_ = [a]\n",
    "    for i in range(len(self.weights)):\n",
    "      z = a @ self.weights[i] + self.biases[i]\n",
    "      self.zs.append(z)\n",
    "      if i < len(self.weights) - 1:\n",
    "        a = self.activation(z)\n",
    "      else:\n",
    "        a = self.softmax(z)\n",
    "      self.as_.append(a)\n",
    "    return a\n",
    "\n",
    "  def compute_loss(self, y_pred, y_true):\n",
    "    eps = 1e-9\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "  def backward(self, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    grads_w = [None] * len(self.weights)\n",
    "    grads_b = [None] * len(self.biases)\n",
    "    delta = (self.as_[-1] - y_true) / m\n",
    "    for i in range(len(self.weights) - 1, -1, -1):\n",
    "      grads_w[i] = self.as_[i].T @ delta\n",
    "      grads_b[i] = np.sum(delta, axis=0, keepdims=True)\n",
    "      if i > 0:\n",
    "        delta = (delta @ self.weights[i].T) * self.activation_grad(self.zs[i - 1])\n",
    "    return grads_w, grads_b\n",
    "\n",
    "  def update_parameters(self, grads_w, grads_b):\n",
    "    for i in range(len(self.weights)):\n",
    "      self.weights[i] -= self.lr * grads_w[i]\n",
    "      self.biases[i] -= self.lr * grads_b[i]\n",
    "\n",
    "  def predict(self, x):\n",
    "    probs = self.forward(x)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "  def evaluate(self, loader):\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images_np, labels_np in to_numpy_loader(loader):\n",
    "      x = normalize(images_np).reshape(images_np.shape[0], -1)\n",
    "      y = one_hot(labels_np)\n",
    "      y_pred = self.forward(x)\n",
    "      losses.append(self.compute_loss(y_pred, y))\n",
    "      preds = np.argmax(y_pred, axis=1)\n",
    "      correct += np.sum(preds == labels_np)\n",
    "      total += labels_np.shape[0]\n",
    "    return float(np.mean(losses)), float(correct / total)\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "  history = []\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images_np, labels_np in to_numpy_loader(train_loader):\n",
    "      x = normalize(images_np).reshape(images_np.shape[0], -1)\n",
    "      y = one_hot(labels_np)\n",
    "      y_pred = model.forward(x)\n",
    "      loss = model.compute_loss(y_pred, y)\n",
    "      grads_w, grads_b = model.backward(y)\n",
    "      model.update_parameters(grads_w, grads_b)\n",
    "      train_losses.append(loss)\n",
    "      train_correct += np.sum(np.argmax(y_pred, axis=1) == labels_np)\n",
    "      train_total += labels_np.shape[0]\n",
    "    train_loss = float(np.mean(train_losses))\n",
    "    train_acc = float(train_correct / train_total)\n",
    "    val_loss, val_acc = model.evaluate(val_loader)\n",
    "    history.append((epoch, train_loss, train_acc, val_loss, val_acc))\n",
    "  return history\n",
    "\n",
    "def run_experiment(config, epochs=5):\n",
    "  model = NeuralNetwork(784, config[\"hidden_sizes\"], 10, activation=config[\"activation\"], lr=config[\"lr\"]) \n",
    "  history = train(model, train_loader, val_loader, epochs)\n",
    "  return model, history\n",
    "\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a506f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install torch torchvision --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c24927",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m all_results = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m   model, history = \u001b[43mrun_experiment\u001b[49m(cfg, epochs=\u001b[32m5\u001b[39m)\n\u001b[32m     11\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m epoch, tr_loss, tr_acc, va_loss, va_acc \u001b[38;5;129;01min\u001b[39;00m history:\n\u001b[32m     12\u001b[39m     all_results.append([cfg[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], epoch, tr_loss, tr_acc, va_loss, va_acc])\n",
      "\u001b[31mNameError\u001b[39m: name 'run_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "  {\"name\": \"relu_2x128\", \"hidden_sizes\": [128, 128], \"activation\": \"relu\", \"lr\": 0.01},\n",
    "  {\"name\": \"relu_3x256\", \"hidden_sizes\": [256, 256, 256], \"activation\": \"relu\", \"lr\": 0.01},\n",
    "  {\"name\": \"sigmoid_2x256\", \"hidden_sizes\": [256, 256], \"activation\": \"sigmoid\", \"lr\": 0.05},\n",
    "  {\"name\": \"tanh_2x256\", \"hidden_sizes\": [256, 256], \"activation\": \"tanh\", \"lr\": 0.01}\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for cfg in experiments:\n",
    "  model, history = run_experiment(cfg, epochs=5)\n",
    "  for epoch, tr_loss, tr_acc, va_loss, va_acc in history:\n",
    "    all_results.append([cfg[\"name\"], epoch, tr_loss, tr_acc, va_loss, va_acc])\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(all_results, columns=[\"experiment\", \"epoch\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"]) \n",
    "results_df.to_csv(\"outputs/experiment_results.csv\", index=False)\n",
    "\n",
    "for name, group in results_df.groupby(\"experiment\"):\n",
    "  plt.figure()\n",
    "  plt.plot(group[\"epoch\"], group[\"train_loss\"], label=\"train_loss\")\n",
    "  plt.plot(group[\"epoch\"], group[\"val_loss\"], label=\"val_loss\")\n",
    "  plt.title(name)\n",
    "  plt.xlabel(\"epoch\")\n",
    "  plt.ylabel(\"loss\")\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"outputs/{name}_loss.png\")\n",
    "  plt.close()\n",
    "  plt.figure()\n",
    "  plt.plot(group[\"epoch\"], group[\"train_acc\"], label=\"train_acc\")\n",
    "  plt.plot(group[\"epoch\"], group[\"val_acc\"], label=\"val_acc\")\n",
    "  plt.title(name)\n",
    "  plt.xlabel(\"epoch\")\n",
    "  plt.ylabel(\"accuracy\")\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"outputs/{name}_acc.png\")\n",
    "  plt.close()\n",
    "\n",
    "results_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
